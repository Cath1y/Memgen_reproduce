[34m[1mwandb[0m: Detected [huggingface_hub.inference] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
  1%|â–‰                                                                                                   | 2/210 [03:08<5:24:35, 93.63s/it]Traceback (most recent call last):
{'loss': 0.0057, 'grad_norm': 0.38850319385528564, 'learning_rate': 0.0, 'num_tokens': 68067.0, 'completions/mean_length': 456.0859375, 'completions/min_length': 100.0, 'completions/max_length': 512.0, 'augmentations/mean_length': 3.3671875, 'augmentations/min_length': 0.0, 'augmentations/max_length': 5.0, 'completions/clipped_ratio': 0.703125, 'completions/mean_terminated_length': 323.65789794921875, 'completions/min_terminated_length': 100.0, 'completions/max_terminated_length': 507.0, 'rewards/compute_reward/mean': 0.578125, 'rewards/compute_reward/std': 0.4957992732524872, 'reward': 0.578125, 'reward_std': 0.19568344950675964, 'frac_reward_zero_std': 0.5625, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.0}
{'loss': 0.0054, 'grad_norm': 0.6007781624794006, 'learning_rate': 4.7619047619047623e-07, 'num_tokens': 128628.0, 'completions/mean_length': 392.5703125, 'completions/min_length': 114.0, 'completions/max_length': 512.0, 'augmentations/mean_length': 3.3515625, 'augmentations/min_length': 0.0, 'augmentations/max_length': 5.0, 'completions/clipped_ratio': 0.46875, 'completions/mean_terminated_length': 287.1911926269531, 'completions/min_terminated_length': 114.0, 'completions/max_terminated_length': 497.0, 'rewards/compute_reward/mean': 0.6015625, 'rewards/compute_reward/std': 0.4915000796318054, 'reward': 0.6015625, 'reward_std': 0.2845958471298218, 'frac_reward_zero_std': 0.375, 'clip_ratio/low_mean': 0.0, 'clip_ratio/low_min': 0.0, 'clip_ratio/high_mean': 0.0, 'clip_ratio/high_max': 0.0, 'clip_ratio/region_mean': 0.0, 'epoch': 0.01}
  File "/root/.vscode-server/memgen/MemGen/main.py", line 115, in <module>
    main()
  File "/root/.vscode-server/memgen/MemGen/main.py", line 109, in main
    runner.train()
  File "/root/.vscode-server/memgen/MemGen/memgen/runner.py", line 216, in train
    self._train_trigger()
  File "/root/.vscode-server/memgen/MemGen/memgen/runner.py", line 198, in _train_trigger
    trigger_trainer.train()
  File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/trainer.py", line 2238, in train
    return inner_training_loop(
  File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/trainer.py", line 2582, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/trainer.py", line 3790, in training_step
    inputs = self._prepare_inputs(inputs)
  File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/trl/extras/profiling.py", line 98, in wrapper
    return func(self, *args, **kwargs)
  File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 1279, in _prepare_inputs
    generation_batch = self._generate_and_score_completions(generation_batch)
  File "/root/.vscode-server/memgen/MemGen/memgen/trainer/trigger_grpo_trainer.py", line 243, in _generate_and_score_completions
    prompt_completion_ids, augmentation_mask = unwrapped_model.generate(
  File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/root/.vscode-server/memgen/MemGen/memgen/model/modeling_memgen.py", line 509, in generate
    outputs = reasoner(
  File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/utils/generic.py", line 959, in wrapper
    output = func(self, *args, **kwargs)
  File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 450, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/utils/generic.py", line 1083, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 379, in forward
    hidden_states = decoder_layer(
  File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
    return super().__call__(*args, **kwargs)
  File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 231, in forward
    hidden_states, _ = self.self_attn(
  File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 167, in forward
    attn_output, attn_weights = attention_interface(
  File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/integrations/flash_attention.py", line 64, in flash_attention_forward
    attn_output = _flash_attention_forward(
  File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/modeling_flash_attention_utils.py", line 643, in _flash_attention_forward
    q, k, v, indices_q, (cu_seq_lens_q, cu_seq_lens_k), (max_length_q, max_length_k) = _upad_input(
  File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/modeling_flash_attention_utils.py", line 280, in _upad_input
    indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(attention_mask)
  File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/modeling_flash_attention_utils.py", line 227, in _get_unpad_data
    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()
KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/root/.vscode-server/memgen/MemGen/main.py", line 115, in <module>
[rank0]:     main()
[rank0]:   File "/root/.vscode-server/memgen/MemGen/main.py", line 109, in main
[rank0]:     runner.train()
[rank0]:   File "/root/.vscode-server/memgen/MemGen/memgen/runner.py", line 216, in train
[rank0]:     self._train_trigger()
[rank0]:   File "/root/.vscode-server/memgen/MemGen/memgen/runner.py", line 198, in _train_trigger
[rank0]:     trigger_trainer.train()
[rank0]:   File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/trainer.py", line 2238, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/trainer.py", line 2582, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/trainer.py", line 3790, in training_step
[rank0]:     inputs = self._prepare_inputs(inputs)
[rank0]:   File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/trl/extras/profiling.py", line 98, in wrapper
[rank0]:     return func(self, *args, **kwargs)
[rank0]:   File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py", line 1279, in _prepare_inputs
[rank0]:     generation_batch = self._generate_and_score_completions(generation_batch)
[rank0]:   File "/root/.vscode-server/memgen/MemGen/memgen/trainer/trigger_grpo_trainer.py", line 243, in _generate_and_score_completions
[rank0]:     prompt_completion_ids, augmentation_mask = unwrapped_model.generate(
[rank0]:   File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/root/.vscode-server/memgen/MemGen/memgen/model/modeling_memgen.py", line 509, in generate
[rank0]:     outputs = reasoner(
[rank0]:   File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/utils/generic.py", line 959, in wrapper
[rank0]:     output = func(self, *args, **kwargs)
[rank0]:   File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 450, in forward
[rank0]:     outputs: BaseModelOutputWithPast = self.model(
[rank0]:   File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/utils/generic.py", line 1083, in wrapper
[rank0]:     outputs = func(self, *args, **kwargs)
[rank0]:   File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 379, in forward
[rank0]:     hidden_states = decoder_layer(
[rank0]:   File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/modeling_layers.py", line 93, in __call__
[rank0]:     return super().__call__(*args, **kwargs)
[rank0]:   File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 231, in forward
[rank0]:     hidden_states, _ = self.self_attn(
[rank0]:   File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 167, in forward
[rank0]:     attn_output, attn_weights = attention_interface(
[rank0]:   File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/integrations/flash_attention.py", line 64, in flash_attention_forward
[rank0]:     attn_output = _flash_attention_forward(
[rank0]:   File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/modeling_flash_attention_utils.py", line 643, in _flash_attention_forward
[rank0]:     q, k, v, indices_q, (cu_seq_lens_q, cu_seq_lens_k), (max_length_q, max_length_k) = _upad_input(
[rank0]:   File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/modeling_flash_attention_utils.py", line 280, in _upad_input
[rank0]:     indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(attention_mask)
[rank0]:   File "/root/miniconda3/envs/memgen/lib/python3.10/site-packages/transformers/modeling_flash_attention_utils.py", line 227, in _get_unpad_data
[rank0]:     indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()
[rank0]: KeyboardInterrupt
